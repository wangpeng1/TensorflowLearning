{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM与二分K均值算法的区别之一：支持向量机（SVM）是一种分类算法，二分k均值算法属于一种聚类算法。\n",
    "SVM与二分K均值算法的区别之一：支持向量机（SVM）是一种监督分类算法，二分k均值算法属于一种非监督分类算法。\n",
    "分类需要标签 聚类不需要\n",
    "常用的分类算法包括：决策树分类法，朴素贝叶斯分类算法(native Bayesian classifier)、基于支持向量机(SVM)的分类器，神经网络法，k-最近邻法(k-nearestneighbor，kNN)，模糊分类法\n",
    "\n",
    "谈谈回归和分类的区别：\n",
    "总的来说两个问题本质上都是一致的，就是模型的拟合（匹配）。 但是分类问题的y值(也称为label), 更离散化一些. 而且， 同一个y值可能对应着一大批的x,  这些x是具有一定范围的。 \n",
    "所以分类问题更多的是 (一定区域的一些x) 对应 着 (一个y).   而回归问题的模型更倾向于 (很小区域内的x，或者一般是一个x)  对应着  (一个y).\n",
    "怎么判定一个人是高富帅还是吊丝? 分类\n",
    "回归分析常用于分析两个变量X和Y 之间的关系。 比如 X＝房子大小 和 Y＝房价 之间的关系， X=(公园人流量，公园门票票价） 与 Y=(公园收入) 之间的关系等等。 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow_cookbook-master/05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors/02_nearest_neighbor.ipynb\n",
    "# 最近邻居算法 聚类\n",
    "distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)\n",
    "#distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), reduction_indices=1))\n",
    "#prediction = tf.arg_min(distance, 0)\n",
    "top_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\n",
    "x_sums = tf.expand_dims(tf.reduce_sum(top_k_xvals, 1),1)\n",
    "x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\n",
    "x_val_weights = tf.expand_dims(tf.div(top_k_xvals,x_sums_repeated), 1)\n",
    "\n",
    "top_k_yvals = tf.gather(y_target_train, top_k_indices)\n",
    "prediction = tf.squeeze(tf.matmul(x_val_weights,top_k_yvals), axis=[1])\n",
    "#prediction = tf.reduce_mean(top_k_yvals, 1)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)\n",
    "\n",
    "# Calculate how many loops over training data\n",
    "num_loops = int(np.ceil(len(x_vals_test)/batch_size))\n",
    "\n",
    "for i in range(num_loops):\n",
    "    min_index = i*batch_size\n",
    "    max_index = min((i+1)*batch_size,len(x_vals_train))\n",
    "    x_batch = x_vals_test[min_index:max_index]\n",
    "    y_batch = y_vals_test[min_index:max_index]\n",
    "    predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n",
    "                                         y_target_train: y_vals_train, y_target_test: y_batch})\n",
    "    batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,\n",
    "                                         y_target_train: y_vals_train, y_target_test: y_batch})\n",
    "\n",
    "    print('Batch #' + str(i+1) + ' MSE: ' + str(np.round(batch_mse,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start a New Graph Session\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "a = tf.Variable(tf.constant(1.))\n",
    "b = tf.Variable(tf.constant(1.))\n",
    "x_val = 5.\n",
    "x_data = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "two_gate = tf.add(tf.multiply(a, x_data), b)\n",
    "\n",
    "# Declare the loss function as the difference between\n",
    "# the output and a target value, 50.\n",
    "loss = tf.square(tf.subtract(two_gate, 50.))\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Run loop across gate\n",
    "print('\\nOptimizing Two Gate Output to 50.')\n",
    "for i in range(10):\n",
    "    sess.run(train_step, feed_dict={x_data: x_val})\n",
    "    a_val, b_val = (sess.run(a), sess.run(b))\n",
    "    two_gate_output = sess.run(two_gate, feed_dict={x_data: x_val})\n",
    "    print(str(a_val) + ' * ' + str(x_val) + ' + ' + str(b_val) + ' = ' + str(two_gate_output))\n",
    "    \"\"\"\n",
    "    Optimizing Two Gate Output to 50.\n",
    "5.4 * 5.0 + 1.88 = 28.88\n",
    "7.512 * 5.0 + 2.3024 = 39.8624\n",
    "8.52576 * 5.0 + 2.50515 = 45.134\n",
    "9.01236 * 5.0 + 2.60247 = 47.6643\n",
    "9.24593 * 5.0 + 2.64919 = 48.8789\n",
    "9.35805 * 5.0 + 2.67161 = 49.4619\n",
    "9.41186 * 5.0 + 2.68237 = 49.7417\n",
    "9.43769 * 5.0 + 2.68754 = 49.876\n",
    "9.45009 * 5.0 + 2.69002 = 49.9405\n",
    "9.45605 * 5.0 + 2.69121 = 49.9714\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/06_Neural_Networks/03_Working_with_Activation_Functions/03_activation_functions.ipynb\n",
    "# Start Graph Session \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "tf.set_random_seed(5)\n",
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "a1 = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "b1 = tf.Variable(tf.random_uniform(shape=[1,1]))\n",
    "a2 = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "b2 = tf.Variable(tf.random_uniform(shape=[1,1]))\n",
    "x = np.random.normal(2, 0.1, 500)\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "sigmoid_activation = tf.sigmoid(tf.add(tf.matmul(x_data, a1), b1))\n",
    "\n",
    "relu_activation = tf.nn.relu(tf.add(tf.matmul(x_data, a2), b2))\n",
    "\n",
    "# Declare the loss function as the difference between\n",
    "# the output and a target value, 0.75.\n",
    "loss1 = tf.reduce_mean(tf.square(tf.subtract(sigmoid_activation, 0.75)))\n",
    "loss2 = tf.reduce_mean(tf.square(tf.subtract(relu_activation, 0.75)))\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step_sigmoid = my_opt.minimize(loss1)\n",
    "train_step_relu = my_opt.minimize(loss2)\n",
    "\n",
    "# Run loop across gate\n",
    "print('\\nOptimizing Sigmoid AND Relu Output to 0.75')\n",
    "loss_vec_sigmoid = []\n",
    "loss_vec_relu = []\n",
    "for i in range(500):\n",
    "    rand_indices = np.random.choice(len(x), size=batch_size)\n",
    "    x_vals = np.transpose([x[rand_indices]])\n",
    "    sess.run(train_step_sigmoid, feed_dict={x_data: x_vals})\n",
    "    sess.run(train_step_relu, feed_dict={x_data: x_vals})\n",
    "    \n",
    "    loss_vec_sigmoid.append(sess.run(loss1, feed_dict={x_data: x_vals}))\n",
    "    loss_vec_relu.append(sess.run(loss2, feed_dict={x_data: x_vals}))    \n",
    "    \n",
    "    sigmoid_output = np.mean(sess.run(sigmoid_activation, feed_dict={x_data: x_vals}))\n",
    "    relu_output = np.mean(sess.run(relu_activation, feed_dict={x_data: x_vals}))\n",
    "    \n",
    "    if i%50==0:\n",
    "        print('sigmoid = ' + str(np.mean(sigmoid_output)) + ' relu = ' + str(np.mean(relu_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  NN网络 根据花的特征 分类什么花\n",
    "## loss function (MSE) tf.reduce_mean(tf.square(y_target - final_output))\n",
    "## 统计loss 用的是 loss_vec.append(np.sqrt(temp_loss))，上面直接统计loss，但是 np.mean的激活函数\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "iris = datasets.load_iris()\n",
    "x_vals = np.array([x[0:3] for x in iris.data])\n",
    "y_vals = np.array([x[3] for x in iris.data])\n",
    "\n",
    "# Create graph session \n",
    "sess = tf.Session()\n",
    "\n",
    "# make results reproducible\n",
    "seed = 2\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)  \n",
    "\n",
    "# Split data into train/test = 80%/20%\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]\n",
    "\n",
    "# Normalize by column (min-max norm)\n",
    "def normalize_cols(m):\n",
    "    col_max = m.max(axis=0)\n",
    "    col_min = m.min(axis=0)\n",
    "    return (m-col_min) / (col_max - col_min)\n",
    "    \n",
    "x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n",
    "x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n",
    "\n",
    "# Declare batch size\n",
    "batch_size = 50\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[None, 3], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Create variables for both NN layers\n",
    "hidden_layer_nodes = 10\n",
    "A1 = tf.Variable(tf.random_normal(shape=[3,hidden_layer_nodes])) # inputs -> hidden nodes\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   # one biases for each hidden node\n",
    "A2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,1])) # hidden inputs -> 1 output\n",
    "b2 = tf.Variable(tf.random_normal(shape=[1]))   # 1 bias for the output\n",
    "\n",
    "\n",
    "# Declare model operations\n",
    "hidden_output = tf.nn.relu(tf.add(tf.matmul(x_data, A1), b1))\n",
    "final_output = tf.nn.relu(tf.add(tf.matmul(hidden_output, A2), b2))\n",
    "\n",
    "# Declare loss function (MSE)\n",
    "loss = tf.reduce_mean(tf.square(y_target - final_output))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.005)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "test_loss = []\n",
    "for i in range(500):\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    rand_x = x_vals_train[rand_index]\n",
    "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(np.sqrt(temp_loss))\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n",
    "    test_loss.append(np.sqrt(test_temp_loss))\n",
    "    if (i+1)%50==0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tensorflow_cookbook-master/06_Neural_Networks/05_Implementing_Different_Layers/05_implementing_different_layers.ipynb 全连接层各种层\n",
    "# Reset Graph\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# parameters for the run\n",
    "row_size = 10\n",
    "col_size = 10\n",
    "conv_size = 2\n",
    "conv_stride_size = 2\n",
    "maxpool_size = 2\n",
    "maxpool_stride_size = 1\n",
    "\n",
    "\n",
    "# ensure reproducibility\n",
    "seed=13\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#Generate 2D data\n",
    "data_size = [row_size,col_size]\n",
    "data_2d = np.random.normal(size=data_size)\n",
    "\n",
    "#--------Placeholder--------\n",
    "x_input_2d = tf.placeholder(dtype=tf.float32, shape=data_size)\n",
    "\n",
    "# Convolution\n",
    "def conv_layer_2d(input_2d, my_filter,stride_size):\n",
    "    # TensorFlow's 'conv2d()' function only works with 4D arrays:\n",
    "    # [batch#, width, height, channels], we have 1 batch, and\n",
    "    # 1 channel, but we do have width AND height this time.\n",
    "    # So next we create the 4D array by inserting dimension 1's.\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Note the stride difference below!\n",
    "    convolution_output = tf.nn.conv2d(input_4d, filter=my_filter, \n",
    "                                      strides=[1,stride_size,stride_size,1], padding=\"VALID\")\n",
    "    # Get rid of unnecessary dimensions\n",
    "    conv_output_2d = tf.squeeze(convolution_output)\n",
    "    return(conv_output_2d)\n",
    "\n",
    "# Create Convolutional Filter\n",
    "my_filter = tf.Variable(tf.random_normal(shape=[conv_size,conv_size,1,1]))\n",
    "# Create Convolutional Layer\n",
    "my_convolution_output = conv_layer_2d(x_input_2d, my_filter,stride_size=conv_stride_size)\n",
    "\n",
    "#--------Activation--------\n",
    "def activation(input_1d):\n",
    "    return(tf.nn.relu(input_1d))\n",
    "\n",
    "# Create Activation Layer\n",
    "my_activation_output = activation(my_convolution_output)\n",
    "\n",
    "#--------Max Pool--------\n",
    "def max_pool(input_2d, width, height,stride):\n",
    "    # Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
    "    # [batch_size=1, width=given, height=given, channels=1]\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Perform the max pooling with strides = [1,1,1,1]\n",
    "    # If we wanted to increase the stride on our data dimension, say by\n",
    "    # a factor of '2', we put strides = [1, 2, 2, 1]\n",
    "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],\n",
    "                                 strides=[1, stride, stride, 1],\n",
    "                                 padding='VALID')\n",
    "    # Get rid of unnecessary dimensions\n",
    "    pool_output_2d = tf.squeeze(pool_output)\n",
    "    return(pool_output_2d)\n",
    "\n",
    "# Create Max-Pool Layer\n",
    "my_maxpool_output = max_pool(my_activation_output, \n",
    "                             width=maxpool_size, height=maxpool_size,stride=maxpool_stride_size)\n",
    "\n",
    "\n",
    "#--------Fully Connected--------\n",
    "def fully_connected(input_layer, num_outputs):\n",
    "    # In order to connect our whole W byH 2d array, we first flatten it out to\n",
    "    # a W times H 1D array.\n",
    "    flat_input = tf.reshape(input_layer, [-1])\n",
    "    # We then find out how long it is, and create an array for the shape of\n",
    "    # the multiplication weight = (WxH) by (num_outputs)\n",
    "    weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input),[num_outputs]]))\n",
    "    # Initialize the weight\n",
    "    weight = tf.random_normal(weight_shape, stddev=0.1)\n",
    "    # Initialize the bias\n",
    "    bias = tf.random_normal(shape=[num_outputs])\n",
    "    # Now make the flat 1D array into a 2D array for multiplication\n",
    "    input_2d = tf.expand_dims(flat_input, 0)\n",
    "    # Multiply and add the bias\n",
    "    full_output = tf.add(tf.matmul(input_2d, weight), bias)\n",
    "    # Get rid of extra dimension\n",
    "    full_output_2d = tf.squeeze(full_output)\n",
    "    return(full_output_2d)\n",
    "\n",
    "# Create Fully Connected Layer\n",
    "my_full_output = fully_connected(my_maxpool_output, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 多个全连接层 进行预测 小孩的出生体重\n",
    "# /tensorflow_cookbook-master/06_Neural_Networks/06_Using_Multiple_Layers/06_using_a_multiple_layer_network.ipynb\n",
    "\n",
    "# Extract y-target (birth weight)\n",
    "y_vals = np.array([x[8] for x in birth_data])\n",
    "\n",
    "# Filter for features of interest\n",
    "cols_of_interest = ['AGE', 'LWT', 'RACE', 'SMOKE', 'PTL', 'HT', 'UI']\n",
    "x_vals = np.array([[x[ix] for ix, feature in enumerate(birth_header) if feature in cols_of_interest] for x in birth_data])\n",
    "# reset the graph for new run\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Create graph session \n",
    "sess = tf.Session()\n",
    "\n",
    "# set batch size for training\n",
    "batch_size = 100\n",
    "\n",
    "# make results reproducible\n",
    "seed = 3\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Split data into train/test = 80%/20%\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]\n",
    "\n",
    "\n",
    "# Normalize by column (min-max norm to be between 0 and 1)\n",
    "def normalize_cols(m):\n",
    "    col_max = m.max(axis=0)\n",
    "    col_min = m.min(axis=0)\n",
    "    return (m-col_min) / (col_max - col_min)\n",
    "    \n",
    "x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n",
    "x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n",
    "\n",
    "\n",
    "# Define Variable Functions (weights and bias)\n",
    "def init_weight(shape, st_dev):\n",
    "    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(weight)\n",
    "    \n",
    "\n",
    "def init_bias(shape, st_dev):\n",
    "    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n",
    "    return(bias)\n",
    "    \n",
    "    \n",
    "# Create Placeholders\n",
    "x_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Create a fully connected layer:\n",
    "def fully_connected(input_layer, weights, biases):\n",
    "    layer = tf.add(tf.matmul(input_layer, weights), biases)\n",
    "    return(tf.nn.relu(layer))\n",
    "\n",
    "\n",
    "#--------Create the first layer (50 hidden nodes)--------\n",
    "weight_1 = init_weight(shape=[7, 25], st_dev=10.0)\n",
    "bias_1 = init_bias(shape=[25], st_dev=10.0)\n",
    "layer_1 = fully_connected(x_data, weight_1, bias_1)\n",
    "\n",
    "#--------Create second layer (25 hidden nodes)--------\n",
    "weight_2 = init_weight(shape=[25, 10], st_dev=10.0)\n",
    "bias_2 = init_bias(shape=[10], st_dev=10.0)\n",
    "layer_2 = fully_connected(layer_1, weight_2, bias_2)\n",
    "\n",
    "\n",
    "#--------Create third layer (5 hidden nodes)--------\n",
    "weight_3 = init_weight(shape=[10, 3], st_dev=10.0)\n",
    "bias_3 = init_bias(shape=[3], st_dev=10.0)\n",
    "layer_3 = fully_connected(layer_2, weight_3, bias_3)\n",
    "\n",
    "\n",
    "#--------Create output layer (1 output value)--------\n",
    "weight_4 = init_weight(shape=[3, 1], st_dev=10.0)\n",
    "bias_4 = init_bias(shape=[1], st_dev=10.0)\n",
    "final_output = fully_connected(layer_3, weight_4, bias_4)\n",
    "\n",
    "# Declare loss function (L1)\n",
    "loss = tf.reduce_mean(tf.abs(y_target - final_output))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.AdamOptimizer(0.05)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "test_loss = []\n",
    "for i in range(200):\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    rand_x = x_vals_train[rand_index]\n",
    "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n",
    "    test_loss.append(test_temp_loss)\n",
    "    if (i+1) % 25 == 0:\n",
    "        print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  r/06_Neural_Networks/07_Improving_Linear_Regression/07_improving_linear_regression.ipynb\n",
    "# 优化线性回归 使用多次迭代ax+b，同时通过方法 综合各个层\n",
    "# 最后一层 不需要用tf.nn.sigmoid 因为loss函数会加上去，但是在统计准确率的时候 需要自己加上 这样才能和Y比较\n",
    "# Create graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Create variable definition\n",
    "def init_variable(shape):\n",
    "    return(tf.Variable(tf.random_normal(shape=shape)))\n",
    "\n",
    "\n",
    "# Create a logistic layer definition\n",
    "def logistic(input_layer, multiplication_weight, bias_weight, activation = True):\n",
    "    linear_layer = tf.add(tf.matmul(input_layer, multiplication_weight), bias_weight)\n",
    "    # We separate the activation at the end because the loss function will\n",
    "    # implement the last sigmoid necessary\n",
    "    if activation:\n",
    "        return(tf.nn.sigmoid(linear_layer))\n",
    "    else:\n",
    "        return(linear_layer)\n",
    "\n",
    "\n",
    "# First logistic layer (7 inputs to 7 hidden nodes)\n",
    "A1 = init_variable(shape=[7,14])\n",
    "b1 = init_variable(shape=[14])\n",
    "logistic_layer1 = logistic(x_data, A1, b1)\n",
    "\n",
    "# Second logistic layer (7 hidden inputs to 5 hidden nodes)\n",
    "A2 = init_variable(shape=[14,5])\n",
    "b2 = init_variable(shape=[5])\n",
    "logistic_layer2 = logistic(logistic_layer1, A2, b2)\n",
    "\n",
    "# Final output layer (5 hidden nodes to 1 output)\n",
    "A3 = init_variable(shape=[5,1])\n",
    "b3 = init_variable(shape=[1])\n",
    "final_output = logistic(logistic_layer2, A3, b3, activation=False)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=final_output, labels=y_target))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.AdamOptimizer(learning_rate = 0.002)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.nn.sigmoid(final_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(1500):\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    rand_x = x_vals_train[rand_index]\n",
    "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    temp_acc_train = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])})\n",
    "    train_acc.append(temp_acc_train)\n",
    "    temp_acc_test = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n",
    "    test_acc.append(temp_acc_test)\n",
    "    if (i+1)%150==0:\n",
    "        print('Loss = ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  tensorflow_cookbook-master/08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST/02_introductory_cnn.ipynb \n",
    "# CNN cov2的使用，以及相关参数的使用，优化函数使用了MomentumOptimizer\n",
    "\n",
    "#使用prediction = tf.nn.softmax(model_output) \n",
    "#预测('Actual: ' + str(actuals[i]) + ' Pred: ' + str(predictions[i]),fontsize=10) 是否和真的匹配\n",
    "#rand_y = train_labels[rand_index]\n",
    "# temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "# actuals = rand_y[0:6]\n",
    "#predictions = np.argmax(temp_train_preds,axis=1)[0:6]\n",
    "\n",
    "# Convolutional layer variables\n",
    "conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n",
    "\n",
    "# fully connected variables\n",
    "resulting_width = image_width // (max_pool_size1 * max_pool_size2)\n",
    "resulting_height = image_height // (max_pool_size1 * max_pool_size2)\n",
    "full1_input_size = resulting_width * resulting_height * conv2_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1],\n",
    "                          stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "# Initialize Model Operations\n",
    "def my_conv_net(input_data):\n",
    "    # First Conv-ReLU-MaxPool Layer\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\n",
    "    # Second Conv-ReLU-MaxPool Layer\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "\n",
    "    # Transform Output into a 1xN layer for next fully connected layer\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "    \n",
    "    return(final_model_output)\n",
    "\n",
    "model_output = my_conv_net(x_input)\n",
    "test_model_output = my_conv_net(eval_input)\n",
    "\n",
    "# Declare Loss Function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Create a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "# Create accuracy function\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis=1)\n",
    "    num_correct = np.sum(np.equal(batch_predictions, targets))\n",
    "    return(100. * num_correct/batch_predictions.shape[0])\n",
    "\n",
    "# Create an optimizer\n",
    "my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "# Initialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/08_Convolutional_Neural_Networks/03_CNN_CIFAR10/03_cnn_cifar10.py \n",
    "# CIFAR10的验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CIFAR10的下载 08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures\n",
    "import os\n",
    "import tarfile\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import scipy.misc\n",
    "\n",
    "cifar_link = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "data_dir = 'temp'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download tar file\n",
    "target_file = os.path.join(data_dir, 'cifar-10-python.tar.gz')\n",
    "if not os.path.isfile(target_file):\n",
    "    print('CIFAR-10 file not found. Downloading CIFAR data (Size = 163MB)')\n",
    "    print('This may take a few minutes, please wait.')\n",
    "    filename, headers = urllib.request.urlretrieve(cifar_link, target_file)\n",
    "\n",
    "# Extract into memory\n",
    "tar = tarfile.open(target_file)\n",
    "tar.extractall(path=data_dir)\n",
    "tar.close()\n",
    "objects = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create train image folders\n",
    "train_folder = 'train_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, train_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, train_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "# Create test image folders\n",
    "test_folder = 'validation_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, test_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, test_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Extract images accordingly\n",
    "data_location = os.path.join(data_dir, 'cifar-10-batches-py')\n",
    "train_names = ['data_batch_' + str(x) for x in range(1,6)]\n",
    "test_names = ['test_batch']\n",
    "\n",
    "\n",
    "def load_batch_from_file(file):\n",
    "    file_conn = open(file, 'rb')\n",
    "    image_dictionary = cPickle.load(file_conn, encoding='latin1')\n",
    "    file_conn.close()\n",
    "    return(image_dictionary)\n",
    "\n",
    "\n",
    "def save_images_from_dict(image_dict, folder='data_dir'):\n",
    "    # image_dict.keys() = 'labels', 'filenames', 'data', 'batch_label'\n",
    "    for ix, label in enumerate(image_dict['labels']):\n",
    "        folder_path = os.path.join(data_dir, folder, objects[label])\n",
    "        filename = image_dict['filenames'][ix]\n",
    "        #Transform image data\n",
    "        image_array = image_dict['data'][ix]\n",
    "        image_array.resize([3, 32, 32])\n",
    "        # Save image\n",
    "        output_location = os.path.join(folder_path, filename)\n",
    "        scipy.misc.imsave(output_location,image_array.transpose())\n",
    "\n",
    "# Sort train images\n",
    "for file in train_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=train_folder)\n",
    "\n",
    "# Sort test images\n",
    "for file in test_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=test_folder)\n",
    "    \n",
    "# Create labels file\n",
    "cifar_labels_file = os.path.join(data_dir,'cifar10_labels.txt')\n",
    "print('Writing labels file, {}'.format(cifar_labels_file))\n",
    "with open(cifar_labels_file, 'w') as labels_file:\n",
    "    for item in objects:\n",
    "        labels_file.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tensorflow_cookbook-master/08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle/05_stylenet.ipynb\n",
    "#tensorflow_cookbook-master/08_Convolutional_Neural_Networks/06_Deepdream\n",
    "## style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices/02_using_multiple_devices.py\n",
    "# CPU GPU设置\n",
    "# -*- coding: utf-8 -*-\n",
    "# Using Multiple Devices\n",
    "#----------------------------------\n",
    "#\n",
    "# This function gives us the ways to use\n",
    "#  multiple devices (executors) in TensorFlow.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# To find out where placement occurs, set 'log_device_placement'\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "\n",
    "\n",
    "# If we load a graph and want device placement to be forgotten,\n",
    "#  we set a parameter in our session:\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "sess_soft = tf.Session(config=config)\n",
    "\n",
    "# GPUs\n",
    "#---------------------------------\n",
    "# Note that the GPU must have a compute capability > 3.5 for TF to use.\n",
    "# http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability\n",
    "\n",
    "\n",
    "# Careful with GPU memory allocation, TF never releases it.  TF starts with almost\n",
    "# all of the GPU memory allocated.  We can slowly grow to that limit with an\n",
    "# option setting:\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "sess_grow = tf.Session(config=config)\n",
    "\n",
    "# Also, we can limit the size of GPU memory used, with the following option\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "sess_limited = tf.Session(config=config)\n",
    "\n",
    "\n",
    "# How to set placements on multiple devices.\n",
    "# Here, assume we have three devies CPU:0, GPU:0, and GPU:1\n",
    "if tf.test.is_built_with_cuda():\n",
    "    with tf.device('/cpu:0'):\n",
    "        a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3])\n",
    "        b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1])\n",
    "        \n",
    "        with tf.device('/gpu:1'):\n",
    "            c = tf.matmul(a,b)\n",
    "            c = tf.reshape(c, [-1])\n",
    "        \n",
    "        with tf.device('/gpu:2'):\n",
    "            d = tf.matmul(b,a)\n",
    "            flat_d = tf.reshape(d, [-1])\n",
    "        \n",
    "        combined = tf.multiply(c, flat_d)\n",
    "    print(sess.run(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow/03_parallelizing_tensorflow.py \n",
    "# 并行计算\n",
    "# -*- coding: utf-8 -*-\n",
    "# Parallelizing TensorFlow\n",
    "#----------------------------------\n",
    "#\n",
    "# We will show how to use TensorFlow distributed\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# We will setup a local cluster (on localhost)\n",
    "\n",
    "# Cluster for 2 local workers (tasks 0 and 1):\n",
    "cluster = tf.train.ClusterSpec({'local': ['localhost:2222', 'localhost:2223']})\n",
    "# Server definition:\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=1)\n",
    "# Finish and add\n",
    "#server.join()\n",
    "\n",
    "# Have each worker do a task\n",
    "# Worker 0 : create matrices\n",
    "# Worker 1 : calculate sum of all elements\n",
    "mat_dim = 25\n",
    "matrix_list = {}\n",
    "\n",
    "with tf.device('/job:local/task:0'):\n",
    "    for i in range(0, 2):\n",
    "        m_label = 'm_{}'.format(i)\n",
    "        matrix_list[m_label] = tf.random_normal([mat_dim, mat_dim])\n",
    "\n",
    "# Have each worker calculate the Cholesky Decomposition\n",
    "sum_outs = {}\n",
    "with tf.device('/job:local/task:1'):\n",
    "    for i in range(0, 2):\n",
    "        A = matrix_list['m_{}'.format(i)]\n",
    "        sum_outs['m_{}'.format(i)] = tf.reduce_sum(A)\n",
    "\n",
    "    # Sum all the cholesky decompositions\n",
    "    summed_out = tf.add_n(list(sum_outs.values()))\n",
    "\n",
    "with tf.Session(server.target) as sess:\n",
    "    result = sess.run(summed_out)\n",
    "    print('Summed Values:{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 调试信息 和 宏\n",
    "# tensorflow_cookbook-master/10_Taking_TensorFlow_to_Production/04_Production_Tips/04_production_tips_for_tf.py\n",
    "# Instead of tyring argparse and main(), TensorFlow provides an 'app' function\n",
    "#  to handle running and loading of arguments\n",
    "\n",
    "# At the beginning of the file, define the flags.\n",
    "tf.app.flags.DEFINE_string(\"worker_locations\", \"\", \"List of worker addresses.\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_integer('generations', 1000, 'Number of training generations.')\n",
    "tf.app.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')\n",
    "\n",
    "# Need to define a 'main' function for the app to run\n",
    "def main(_):\n",
    "    worker_ips = FLAGS.worker_locations.split(\",\")\n",
    "    learning_rate = FLAGS.learning_rate\n",
    "    generations = FLAGS.generations\n",
    "    run_unit_tests = FLAGS.run_unit_tests\n",
    "\n",
    "# Run the TensorFlow app\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n",
    "\n",
    "\n",
    "# Use of TensorFlow's built in logging:\n",
    "# Five levels: DEBUG, INFO, WARN, ERROR, and FATAL\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "# WARN is the default value, but to see more information, you can set it to\n",
    "#    INFO or DEBUG\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "# Note: 'DEBUG' is quite verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tensorflow_cookbook-master/10_Taking_TensorFlow_to_Production/05_Production_Example/05_production_ex_eval.py\n",
    "## 从已有的恢复\n",
    "# -*- coding: utf-8 -*-\n",
    "# TensorFlow Production Example (Evaluating)\n",
    "#----------------------------------\n",
    "#\n",
    "# We pull together everything and create an example\n",
    "#    of best tensorflow production tips\n",
    "#\n",
    "# The example we will productionalize is the spam/ham RNN\n",
    "#    from the RNN Chapter.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"storage_folder\", \"temp\", \"Where to store model and data.\")\n",
    "tf.app.flags.DEFINE_string('model_file', False, 'Model file location.')\n",
    "tf.app.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return(text_string)\n",
    "\n",
    "\n",
    "# Load vocab processor\n",
    "def load_vocab():\n",
    "    vocab_path = os.path.join(FLAGS.storage_folder, \"vocab\")\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "    return(vocab_processor)\n",
    "\n",
    "\n",
    "# Process input data:\n",
    "def process_data(input_data, vocab_processor):\n",
    "    input_data = clean_text(input_data)\n",
    "    input_data = input_data.split()\n",
    "    processed_input = np.array(list(vocab_processor.transform(input_data)))\n",
    "    return(processed_input)\n",
    "\n",
    "\n",
    "# Get input function\n",
    "def get_input_data():\n",
    "    \"\"\"\n",
    "    For this function, we just prompt the user for a text message to evaluate\n",
    "        But this function could also potentially read a file in as well.\n",
    "    \"\"\"\n",
    "    input_text = input(\"Please enter a text message to evaluate: \")\n",
    "    vocab_processor = load_vocab()\n",
    "    return(process_data(input_text, vocab_processor))\n",
    "\n",
    "\n",
    "# Test clean_text function\n",
    "class clean_test(tf.test.TestCase):\n",
    "    # Make sure cleaning function behaves correctly\n",
    "    def clean_string_test(self):\n",
    "        with self.test_session():\n",
    "            test_input = '--TensorFlow\\'s so Great! Don\\t you think so?   '\n",
    "            test_expected = 'tensorflows so great don you think so'\n",
    "            test_out = clean_text(test_input)\n",
    "            self.assertEqual(test_expected, test_out)\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(args):\n",
    "    # Get flags\n",
    "    storage_folder = FLAGS.storage_folder\n",
    "    \n",
    "    # Get user input text\n",
    "    x_data = get_input_data()\n",
    "    \n",
    "    # Load model\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            # Load the saved meta graph and restore variables\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(os.path.join(storage_folder, \"model.ckpt\")))\n",
    "            saver.restore(sess, os.path.join(storage_folder, \"model.ckpt\"))\n",
    "\n",
    "            # Get the placeholders from the graph by name\n",
    "            x_data_ph = graph.get_operation_by_name(\"x_data_ph\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "            probability_outputs = graph.get_operation_by_name(\"probability_outputs\").outputs[0]\n",
    "\n",
    "            # Make the prediction\n",
    "            eval_feed_dict = {x_data_ph: x_data, dropout_keep_prob: 1.0}\n",
    "            probability_prediction = sess.run(tf.reduce_mean(probability_outputs, 0), eval_feed_dict)\n",
    "            \n",
    "            # Print output (Or save to file or DB connection?)\n",
    "            print('Probability of Spam: {:.4}'.format(probability_prediction[1]))\n",
    "\n",
    "# Run main module/tf App\n",
    "if __name__ == \"__main__\":\n",
    "    if FLAGS.run_unit_tests:\n",
    "        # Perform unit tests\n",
    "        tf.test.main()\n",
    "    else:\n",
    "        # Run evaluation\n",
    "        tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##tensorflow_cookbook-master/10_Taking_TensorFlow_to_Production/05_Production_Example/05_production_ex_train.py\n",
    "# 保存\n",
    "# -*- coding: utf-8 -*-\n",
    "# TensorFlow Production Example (Training)\n",
    "#----------------------------------\n",
    "#\n",
    "# We pull together everything and create an example\n",
    "#    of best tensorflow production tips\n",
    "#\n",
    "# The example we will productionalize is the spam/ham RNN\n",
    "#    from \n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Define App Flags\n",
    "tf.app.flags.DEFINE_string(\"storage_folder\", \"temp\", \"Where to store model and data.\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0005, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_float('dropout_prob', 0.5, 'Per to keep probability for dropout.')\n",
    "tf.app.flags.DEFINE_integer('epochs', 20, 'Number of epochs for training.')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 250, 'Batch Size for training.')\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 20, 'Max sentence length in words.')\n",
    "tf.app.flags.DEFINE_integer('rnn_size', 15, 'RNN feature size.')\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 25, 'Word embedding size.')\n",
    "tf.app.flags.DEFINE_integer('min_word_frequency', 20, 'Word frequency cutoff.')\n",
    "tf.app.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Define how to get data\n",
    "def get_data(storage_folder=FLAGS.storage_folder, data_file=\"text_data.txt\"):\n",
    "    \"\"\"\n",
    "    This function gets the spam/ham data.  It will download it if it doesn't\n",
    "    already exist on disk (at specified folder/file location).\n",
    "    \"\"\"\n",
    "    # Make a storage folder for models and data\n",
    "    if not os.path.exists(storage_folder):\n",
    "        os.makedirs(storage_folder)\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(storage_folder, data_file)):\n",
    "        zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "        r = requests.get(zip_url)\n",
    "        z = ZipFile(io.BytesIO(r.content))\n",
    "        file = z.read('SMSSpamCollection')\n",
    "        # Format Data\n",
    "        text_data = file.decode()\n",
    "        text_data = text_data.encode('ascii',errors='ignore')\n",
    "        text_data = text_data.decode().split('\\n')\n",
    "\n",
    "        # Save data to text file\n",
    "        with open(os.path.join(storage_folder, data_file), 'w') as file_conn:\n",
    "            for text in text_data:\n",
    "                file_conn.write(\"{}\\n\".format(text))\n",
    "    else:\n",
    "        # Open data from text file\n",
    "        text_data = []\n",
    "        with open(os.path.join(storage_folder, data_file), 'r') as file_conn:\n",
    "            for row in file_conn:\n",
    "                text_data.append(row)\n",
    "        text_data = text_data[:-1]\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    [y_data, x_data] = [list(x) for x in zip(*text_data)]\n",
    "    \n",
    "    return(x_data, y_data)\n",
    "\n",
    "\n",
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return(text_string)\n",
    "\n",
    "\n",
    "# Test clean_text function\n",
    "class clean_test(tf.test.TestCase):\n",
    "    # Make sure cleaning function behaves correctly\n",
    "    def clean_string_test(self):\n",
    "        with self.test_session():\n",
    "            test_input = '--TensorFlow\\'s so Great! Don\\t you think so?   '\n",
    "            test_expected = 'tensorflows so great don you think so'\n",
    "            test_out = clean_text(test_input)\n",
    "            self.assertEqual(test_expected, test_out)\n",
    "\n",
    "# Define RNN Model\n",
    "def rnn_model(x_data_ph, max_sequence_length, vocab_size, embedding_size,\n",
    "              rnn_size, dropout_keep_prob):\n",
    "    # Create embedding\n",
    "    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)\n",
    "\n",
    "    # Define the RNN cell\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units = rnn_size)\n",
    "    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
    "    output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "\n",
    "    # Get output of RNN sequence\n",
    "    output = tf.transpose(output, [1, 0, 2])\n",
    "    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "    logits_out = tf.matmul(last, weight) + bias\n",
    "    \n",
    "    return(logits_out)\n",
    "\n",
    "\n",
    "# Define accuracy function\n",
    "def get_accuracy(logits, actuals):\n",
    "    # Calulate if each output is correct\n",
    "    batch_acc = tf.equal(tf.argmax(logits, 1), tf.cast(actuals, tf.int64))\n",
    "    # Convert logical to float\n",
    "    batch_acc = tf.cast(batch_acc, tf.float32)\n",
    "    return(batch_acc)\n",
    "\n",
    "# Define main program\n",
    "def main(args):\n",
    "    # Set verbosity to get more information from TensorFlow\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    # Create a visualizer object for Tensorboard viewing\n",
    "    summary_writer = tf.summary.FileWriter('tensorboard', tf.get_default_graph())\n",
    "    # Create tensorboard folder if not exists\n",
    "    if not os.path.exists('tensorboard'):\n",
    "        os.makedirs('tensorboard')\n",
    "    \n",
    "    # Set model parameters\n",
    "    storage_folder = FLAGS.storage_folder\n",
    "    learning_rate = FLAGS.learning_rate\n",
    "    run_unit_tests = FLAGS.run_unit_tests\n",
    "    epochs = FLAGS.epochs\n",
    "    batch_size = FLAGS.batch_size\n",
    "    max_sequence_length = FLAGS.max_sequence_length\n",
    "    rnn_size = FLAGS.rnn_size\n",
    "    embedding_size = FLAGS.embedding_size\n",
    "    min_word_frequency = FLAGS.min_word_frequency\n",
    "    \n",
    "    # Get text->spam/ham data\n",
    "    x_data, y_data = get_data()\n",
    "    \n",
    "    # Clean texts\n",
    "    x_data = [clean_text(x) for x in x_data]\n",
    "\n",
    "    # Change texts into numeric vectors\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n",
    "                                                                         min_frequency=min_word_frequency)\n",
    "    text_processed = np.array(list(vocab_processor.fit_transform(x_data)))\n",
    "    \n",
    "    # Save vocab processor (for loading and future evaluation)\n",
    "    vocab_processor.save(os.path.join(storage_folder, \"vocab\"))\n",
    "    \n",
    "    # Shuffle and split data\n",
    "    text_processed = np.array(text_processed)\n",
    "    y_data = np.array([1 if x=='ham' else 0 for x in y_data])\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(y_data)))\n",
    "    x_shuffled = text_processed[shuffled_ix]\n",
    "    y_shuffled = y_data[shuffled_ix]\n",
    "\n",
    "    # Split train/test set\n",
    "    ix_cutoff = int(len(y_shuffled)*0.80)\n",
    "    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.Session()\n",
    "        # Define placeholders\n",
    "        x_data_ph = tf.placeholder(tf.int32, [None, max_sequence_length], name='x_data_ph')\n",
    "        y_output_ph = tf.placeholder(tf.int32, [None], name='y_output_ph')\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "        # Define Model\n",
    "        rnn_model_outputs = rnn_model(x_data_ph, max_sequence_length, vocab_size,\n",
    "                                      embedding_size, rnn_size, dropout_keep_prob)\n",
    "\n",
    "        # Prediction\n",
    "        # Although we won't use the following operation, we declare and name\n",
    "        #   the probability outputs so that we can recall them later for evaluation\n",
    "        rnn_prediction = tf.nn.softmax(rnn_model_outputs, name=\"probability_outputs\")\n",
    "        \n",
    "        # Loss function\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rnn_model_outputs, labels=y_output_ph)\n",
    "        # Remember that for this loss function, logits=float32, labels=int32\n",
    "        loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "\n",
    "        # Model Accuracy Operation\n",
    "        accuracy = tf.reduce_mean(get_accuracy(rnn_model_outputs, y_output_ph), name=\"accuracy\")\n",
    "    \n",
    "        # Add scalar summaries for Tensorboard\n",
    "        with tf.name_scope('Scalar_Summaries'):\n",
    "                tf.summary.scalar('Loss', loss)\n",
    "                tf.summary.scalar('Accuracy', accuracy)\n",
    "    \n",
    "        # Declare Optimizer/train step\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        \n",
    "        # Declare summary merging operation\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "        # Create a graph/Variable saving/loading operations\n",
    "        saver = tf.train.Saver()    \n",
    "    \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "    \n",
    "        # Start training\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Shuffle training data\n",
    "            shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "            x_train = x_train[shuffled_ix]\n",
    "            y_train = y_train[shuffled_ix]\n",
    "            num_batches = int(len(x_train)/batch_size) + 1\n",
    "            #\n",
    "            for i in range(num_batches):\n",
    "                # Select train data\n",
    "                min_ix = i * batch_size\n",
    "                max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n",
    "                x_train_batch = x_train[min_ix:max_ix]\n",
    "                y_train_batch = y_train[min_ix:max_ix]\n",
    "        \n",
    "                # Run train step\n",
    "                train_dict = {x_data_ph: x_train_batch,\n",
    "                              y_output_ph: y_train_batch,\n",
    "                              dropout_keep_prob:0.5}\n",
    "                _, summary = sess.run([train_step, summary_op], feed_dict=train_dict)\n",
    "                \n",
    "                summary_writer = tf.summary.FileWriter('tensorboard')\n",
    "                summary_writer.add_summary(summary, i)\n",
    "        \n",
    "            # Run loss and accuracy for training\n",
    "            temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n",
    "\n",
    "            test_dict = {x_data_ph: x_test, y_output_ph: y_test, dropout_keep_prob:1.0}\n",
    "            temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n",
    "            \n",
    "            # Print Epoch Summary\n",
    "            print('Epoch: {}, Train Loss:{:.2}, Train Acc: {:.2}'.format(epoch+1, temp_train_loss, temp_train_acc))\n",
    "            print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))\n",
    "            \n",
    "            # Save model every epoch\n",
    "            saver.save(sess, os.path.join(storage_folder, \"model.ckpt\"))\n",
    "\n",
    "# Run main module/tf App\n",
    "if __name__ == \"__main__\":\n",
    "    if FLAGS.run_unit_tests:\n",
    "        # Perform unit tests\n",
    "        tf.test.main()\n",
    "    else:\n",
    "        # Run evaluation\n",
    "        tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分布式运行\n",
    "# https://github.com/thewintersun/distributeTensorflowExample/blob/master/distribute.py\n",
    "#coding=utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.00003, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_integer('steps_to_validate', 1000,\n",
    "                     'Steps to validate and print loss')\n",
    "\n",
    "# For distributed\n",
    "tf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "tf.app.flags.DEFINE_integer(\"issync\", 0, \"是否采用分布式的同步模式，1表示同步模式，0表示异步模式\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = FLAGS.learning_rate\n",
    "steps_to_validate = FLAGS.steps_to_validate\n",
    "\n",
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n",
    "  server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index)\n",
    "\n",
    "  issync = FLAGS.issync\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "                    worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "                    cluster=cluster)):\n",
    "      global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "      input = tf.placeholder(\"float\")\n",
    "      label = tf.placeholder(\"float\")\n",
    "\n",
    "      weight = tf.get_variable(\"weight\", [1], tf.float32, initializer=tf.random_normal_initializer())\n",
    "      biase  = tf.get_variable(\"biase\", [1], tf.float32, initializer=tf.random_normal_initializer())\n",
    "      pred = tf.multiply(input, weight) + biase\n",
    "\n",
    "      loss_value = loss(label, pred)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "      grads_and_vars = optimizer.compute_gradients(loss_value)\n",
    "      if issync == 1:\n",
    "        #同步模式计算更新梯度\n",
    "        rep_op = tf.train.SyncReplicasOptimizer(optimizer,\n",
    "                                                replicas_to_aggregate=len(\n",
    "                                                  worker_hosts),\n",
    "                                                replica_id=FLAGS.task_index,\n",
    "                                                total_num_replicas=len(\n",
    "                                                  worker_hosts),\n",
    "                                                use_locking=True)\n",
    "        train_op = rep_op.apply_gradients(grads_and_vars,\n",
    "                                       global_step=global_step)\n",
    "        init_token_op = rep_op.get_init_tokens_op()\n",
    "        chief_queue_runner = rep_op.get_chief_queue_runner()\n",
    "      else:\n",
    "        #异步模式计算更新梯度\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars,\n",
    "                                       global_step=global_step)\n",
    "\n",
    "\n",
    "      init_op = tf.initialize_all_variables()\n",
    "      \n",
    "      saver = tf.train.Saver()\n",
    "      tf.summary.scalar('cost', loss_value)\n",
    "      summary_op = tf.summary.merge_all()\n",
    " \n",
    "    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                            logdir=\"./checkpoint/\",\n",
    "                            init_op=init_op,\n",
    "                            summary_op=None,\n",
    "                            saver=saver,\n",
    "                            global_step=global_step,\n",
    "                            save_model_secs=60)\n",
    "\n",
    "    with sv.prepare_or_wait_for_session(server.target) as sess:\n",
    "      # 如果是同步模式\n",
    "      if FLAGS.task_index == 0 and issync == 1:\n",
    "        sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "        sess.run(init_token_op)\n",
    "      step = 0\n",
    "      while  step < 1000000:\n",
    "        train_x = np.random.randn(1)\n",
    "        train_y = 2 * train_x + np.random.randn(1) * 0.33  + 10\n",
    "        _, loss_v, step = sess.run([train_op, loss_value,global_step], feed_dict={input:train_x, label:train_y})\n",
    "        if step % steps_to_validate == 0:\n",
    "          w,b = sess.run([weight,biase])\n",
    "          print(\"step: %d, weight: %f, biase: %f, loss: %f\" %(step, w, b, loss_v))\n",
    "\n",
    "    sv.stop()\n",
    "\n",
    "def loss(label, pred):\n",
    "  return tf.square(label - pred)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CNN LSTM混合  https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "# define CNN model\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(...))\n",
    "cnn.add(MaxPooling2D(...))\n",
    "cnn.add(Flatten())\n",
    "# define LSTM model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(cnn, ...))\n",
    "model.add(LSTM(..))\n",
    "model.add(Dense(...))\n",
    "\n",
    "#或者\n",
    "\n",
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv2D(...))\n",
    "model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define LSTM model\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  靠近Dense的不用 return_sequences=True 这是因为输出的结构不一样\n",
    "## https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "model = Sequential()\n",
    "model.add(LSTM(..., return_sequences=True, input_shape=(...)))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(..., return_sequences=True))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  http://henning.kropponline.de/2017/03/19/distributing-tensorflow/ 异步执行\n",
    "\"\"\"\n",
    " A simple MNIST classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    " \n",
    "import argparse\n",
    "import sys\n",
    " \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    " \n",
    "import tensorflow as tf\n",
    " \n",
    "FLAGS = None\n",
    " \n",
    " \n",
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    " \n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts })\n",
    " \n",
    "  server = tf.train.Server(cluster, \n",
    "                           job_name=FLAGS.job_name,\n",
    "                           task_index=FLAGS.task_index)\n",
    " \n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "    \n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "                       worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "                       cluster=cluster)):\n",
    "      \n",
    "      global_step = tf.contrib.framework.get_or_create_global_step()\n",
    " \n",
    "      with tf.name_scope(\"input\"):\n",
    "        mnist = input_data.read_data_sets(\"./input_data\", one_hot=True)\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name=\"y-input\")\n",
    "      \n",
    "      tf.set_random_seed(1)\n",
    "      with tf.name_scope(\"weights\"):\n",
    "        W = tf.Variable(tf.zeros([784, 10]))\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    " \n",
    "      with tf.name_scope(\"model\"):\n",
    "        y = tf.matmul(x, W) + b\n",
    " \n",
    "      with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    " \n",
    "      with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    " \n",
    "      with tf.name_scope(\"acc\"):\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    " \n",
    "    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                             global_step=global_step,\n",
    "                             init_op=init_op)\n",
    " \n",
    "    with sv.prepare_or_wait_for_session(server.target) as sess:\n",
    "      for _ in range(100):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    " \n",
    "      print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    " \n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  # Flags for defining the tf.train.ClusterSpec\n",
    "  parser.add_argument(\n",
    "      \"--ps_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--worker_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--job_name\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"One of 'ps', 'worker'\"\n",
    "  )\n",
    "  # Flags for defining the tf.train.Server\n",
    "  parser.add_argument(\n",
    "      \"--task_index\",\n",
    "      type=int,\n",
    "      default=0,\n",
    "      help=\"Index of task within the job\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://ischlag.github.io/2016/06/12/async-distributed-tensorflow/ 分布式 里面有图描述两种不同的分布式\n",
    "'''\n",
    "There are different ways to train a network in a distributed fashion. \n",
    "The simplest approach is to share all the model parameters across all workers\n",
    "while parallelising data and gradient updates. In a synchronised setting,several batches are processed at the same time. \n",
    "Once all the workers are done, the parameter updates are averaged and the update is performed only once. \n",
    "In an asynchronised setting, every worker will update the model parameters once it has finished and not wait.\n",
    "I had trouble getting a synchronised setting to work and once I managed to run something it was understandably slower\n",
    "but didn’t perform better.\n",
    "\n",
    "Distributed Tensorflow example of using data parallelism and share model parameters.\n",
    "Trains a simple sigmoid neural network on mnist for 20 epochs on three machines using one parameter server. \n",
    "\n",
    "Change the hardcoded host urls below with your own hosts. \n",
    "Run like this: \n",
    "\n",
    "pc-01$ python example.py --job-name=\"ps\" --task_index=0 \n",
    "pc-02$ python example.py --job-name=\"worker\" --task_index=0 \n",
    "pc-03$ python example.py --job-name=\"worker\" --task_index=1 \n",
    "pc-04$ python example.py --job-name=\"worker\" --task_index=2 \n",
    "\n",
    "More details here: ischlag.github.io\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# cluster specification\n",
    "parameter_servers = [\"pc-01:2222\"]\n",
    "workers = [ \"pc-02:2222\", \n",
    "      \"pc-03:2222\",\n",
    "      \"pc-04:2222\"]\n",
    "cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\n",
    "\n",
    "# input flags\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# start a server for a specific task\n",
    "server = tf.train.Server(cluster, \n",
    "                          job_name=FLAGS.job_name,\n",
    "                          task_index=FLAGS.task_index)\n",
    "\n",
    "# config\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "logs_path = \"/tmp/mnist/1\"\n",
    "\n",
    "# load mnist data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "if FLAGS.job_name == \"ps\":\n",
    "  server.join()\n",
    "elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "  # Between-graph replication\n",
    "  with tf.device(tf.train.replica_device_setter(\n",
    "    worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "    cluster=cluster)):\n",
    "\n",
    "    # count the number of updates\n",
    "    global_step = tf.get_variable('global_step', [], \n",
    "                                initializer = tf.constant_initializer(0), \n",
    "                                trainable = False)\n",
    "\n",
    "    # input images\n",
    "    with tf.name_scope('input'):\n",
    "      # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "      x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n",
    "      # target 10 output classes\n",
    "      y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n",
    "\n",
    "    # model parameters will change during training so we use tf.Variable\n",
    "    tf.set_random_seed(1)\n",
    "    with tf.name_scope(\"weights\"):\n",
    "      W1 = tf.Variable(tf.random_normal([784, 100]))\n",
    "      W2 = tf.Variable(tf.random_normal([100, 10]))\n",
    "\n",
    "    # bias\n",
    "    with tf.name_scope(\"biases\"):\n",
    "      b1 = tf.Variable(tf.zeros([100]))\n",
    "      b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # implement model\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "      # y is our prediction\n",
    "      z2 = tf.add(tf.matmul(x,W1),b1)\n",
    "      a2 = tf.nn.sigmoid(z2)\n",
    "      z3 = tf.add(tf.matmul(a2,W2),b2)\n",
    "      y  = tf.nn.softmax(z3)\n",
    "\n",
    "    # specify cost function\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "      # this is our cost\n",
    "      cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "    # specify optimizer\n",
    "    with tf.name_scope('train'):\n",
    "      # optimizer is an \"operation\" which we can execute in a session\n",
    "      grad_op = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      '''\n",
    "      rep_op = tf.train.SyncReplicasOptimizer(grad_op, \n",
    "                                          replicas_to_aggregate=len(workers),\n",
    "                                          replica_id=FLAGS.task_index, \n",
    "                                          total_num_replicas=len(workers),\n",
    "                                          use_locking=True\n",
    "                                          )\n",
    "      train_op = rep_op.minimize(cross_entropy, global_step=global_step)\n",
    "      '''\n",
    "      train_op = grad_op.minimize(cross_entropy, global_step=global_step)\n",
    "      \n",
    "    '''\n",
    "    init_token_op = rep_op.get_init_tokens_op()\n",
    "    chief_queue_runner = rep_op.get_chief_queue_runner()\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope('Accuracy'):\n",
    "      # accuracy\n",
    "      correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # create a summary for our cost and accuracy\n",
    "    tf.scalar_summary(\"cost\", cross_entropy)\n",
    "    tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "    # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    print(\"Variables initialized ...\")\n",
    "\n",
    "  sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                            global_step=global_step,\n",
    "                            init_op=init_op)\n",
    "\n",
    "  begin_time = time.time()\n",
    "  frequency = 100\n",
    "  with sv.prepare_or_wait_for_session(server.target) as sess:\n",
    "    '''\n",
    "    # is chief\n",
    "    if FLAGS.task_index == 0:\n",
    "      sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "      sess.run(init_token_op)\n",
    "    '''\n",
    "    # create log writer object (this will log on every machine)\n",
    "    writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "        \n",
    "    # perform training cycles\n",
    "    start_time = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "      # number of batches in one epoch\n",
    "      batch_count = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "      count = 0\n",
    "      for i in range(batch_count):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # perform the operations we defined earlier on batch\n",
    "        _, cost, summary, step = sess.run(\n",
    "                        [train_op, cross_entropy, summary_op, global_step], \n",
    "                        feed_dict={x: batch_x, y_: batch_y})\n",
    "        writer.add_summary(summary, step)\n",
    "\n",
    "        count += 1\n",
    "        if count % frequency == 0 or i+1 == batch_count:\n",
    "          elapsed_time = time.time() - start_time\n",
    "          start_time = time.time()\n",
    "          print(\"Step: %d,\" % (step+1), \n",
    "                \" Epoch: %2d,\" % (epoch+1), \n",
    "                \" Batch: %3d of %3d,\" % (i+1, batch_count), \n",
    "                \" Cost: %.4f,\" % cost, \n",
    "                \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\n",
    "          count = 0\n",
    "\n",
    "    print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "    print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n",
    "    print(\"Final Cost: %.4f\" % cost)\n",
    "\n",
    "  sv.stop()\n",
    "  print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    " \n",
    "from multiprocessing import Process\n",
    " \n",
    "N_WORKERS = 3\n",
    "SPEC = {'ps': ['127.0.0.1:12222'], 'worker': ['127.0.0.1:12223', '127.0.0.1:12224', '127.0.0.1:12225']}\n",
    " \n",
    "def run_ps_server():\n",
    "    spec = tf.train.ClusterSpec(SPEC)\n",
    "    ps_server = tf.train.Server(spec, job_name='ps', task_index=0)\n",
    "    ps_server.join()\n",
    "    \n",
    "def run_worker(task):\n",
    "    spec = tf.train.ClusterSpec(SPEC)\n",
    "    server = tf.train.Server(spec, job_name='worker', task_index=task)\n",
    "    with tf.device(tf.train.replica_device_setter(1, worker_device=\"/job:worker/task:%d\" % task)):\n",
    "        global_step = tf.get_variable('global_step', [],\n",
    "                                      initializer = tf.constant_initializer(0),\n",
    "                                      trainable = False)\n",
    "        inc_global_step = tf.assign_add(global_step, 1)\n",
    "        init_op = tf.global_variables_initializer()\n",
    " \n",
    "    sv = tf.train.Supervisor(is_chief=(task == 0),\n",
    "                             global_step=global_step,\n",
    "                             init_op=init_op)\n",
    "    config = tf.ConfigProto(device_filters=[\"/job:ps\", \"/job:worker/task:{}/cpu:0\".format(task)])\n",
    " \n",
    "    with sv.managed_session(server.target, config=config) as sess, sess.as_default():\n",
    "        print 'task {}, global_step {}'.format(task, sess.run(global_step))\n",
    "        if task == 0:\n",
    "            sess.run(inc_global_step)\n",
    "        elif task == 1:\n",
    "            sess.run(inc_global_step)\n",
    "            sess.run(inc_global_step)\n",
    "        print 'task {}, global_step {}'.format(task, sess.run(global_step))\n",
    " \n",
    "    if task == 2:\n",
    "        sv.stop()\n",
    " \n",
    " \n",
    "def main(_):\n",
    "    ps_worker = Process(target=run_ps_server, args=())\n",
    "    ps_worker.daemon = True\n",
    "    ps_worker.start()\n",
    " \n",
    "    worker_processes = []\n",
    "    for i in xrange(N_WORKERS):\n",
    "        time.sleep(0.01)\n",
    "        w = Process(target=run_worker, args=(i,))\n",
    "        w.daemon = True\n",
    "        w.start()\n",
    "        worker_processes.append(w)\n",
    "    for w in worker_processes: w.join()\n",
    " \n",
    "    ps_worker.terminate()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

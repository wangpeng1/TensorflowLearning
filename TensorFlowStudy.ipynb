{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Neg_1:0\", shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1,2]])\n",
    "neg_x = tf.negative(x)\n",
    "print(neg_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -2]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(neg_x)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Neg_1:0\", shape=(1, 2), dtype=int32)\n",
      "[[-1 -2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveSession.close of <tensorflow.python.client.session.InteractiveSession object at 0x0000000007CE8DD8>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##另一种关闭方式##\n",
    "\n",
    "print(neg_x)\n",
    "sess = tf.InteractiveSession()\n",
    "neg_op = tf.negative(x)\n",
    "result = neg_op.eval()\n",
    "\n",
    "print(result)\n",
    "\n",
    "sess.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -2]]\n"
     ]
    }
   ],
   "source": [
    "# 添加参数，配置\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    result = sess.run(neg_op)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike False\n",
      "Spike True\n",
      "Spike False\n",
      "Spike False\n",
      "Spike True\n",
      "Spike False\n",
      "Spike True\n"
     ]
    }
   ],
   "source": [
    "## 使用变量，初始化变量 给变量赋值\n",
    "tf.InteractiveSession()\n",
    "raw_data = [1., 2., 8., -1., 0., 5.5, 6., 13]\n",
    "\n",
    "# 必须初始化变量 \n",
    "spike = tf.Variable(False)\n",
    "# 下面这句可以注释掉\n",
    "spike.initializer.run()\n",
    "\n",
    "# assign(spike, tf.constant(True)) 将后面的值赋值给spike\n",
    "for i in range(1, len(raw_data)):\n",
    "    if raw_data[i] - raw_data[i-1] > 5:\n",
    "        updater = tf.assign(spike, tf.constant(True))\n",
    "        updater.eval()\n",
    "    else:\n",
    "        tf.assign(spike, False).eval()\n",
    "    print(\"Spike\", spike.eval())\n",
    "    \n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##保存变量\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"C:\\\\Users\\\\wilbur.wang\\\\AnacondaProjects\\\\TensorFlow-Book-master\\\\ch02_basics\\\\save\")\n",
    "print(\"spikes data saved in file: %s\" % save_path)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "try:\n",
    "    saver.restore(sess, 'spikes.ckpt')\n",
    "    print(spikes.eval())\n",
    "except:\n",
    "    print('file not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##线程和队列TensorFlow-Book-master/ch02_basics/Concept09_queue.ipynb\n",
    "import multiprocessing\n",
    "NUM_THREADS = multiprocessing.cpu_count()\n",
    "\n",
    "xs = np.random.randn(100, 3)\n",
    "ys = np.random.randint(0, 2, size=100)\n",
    "\n",
    "## 定义队列和初始化\n",
    "queue = tf.FIFOQueue(capacity=1000, dtypes=[tf.float32, tf.int32])\n",
    "\n",
    "## 入列和出列\n",
    "enqueue_op = queue.enqueue_many([xs, ys])\n",
    "x_op, y_op = queue.dequeue()\n",
    "\n",
    "## 开启四个线程http://www.jianshu.com/p/d063804fb272\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "## 参看上面的连接 这个是启动队列线程 需要传sess，还有一个是全局线程\n",
    "enqueue_threads = qr.create_threads(sess, coord=coord, start=True)\n",
    "\n",
    "for _ in range(100):\n",
    "    if coord.should_stop():\n",
    "        break\n",
    "    x, y = sess.run([x_op, y_op])\n",
    "    print(x, y)\n",
    "    \n",
    "coord.request_stop()\n",
    "coord.join(enqueue_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read file http://norvig.com/big.txt\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([\"big.txt\"])\n",
    "reader = tf.TextLineReader()\n",
    "key_op, value_op = reader.read(filename_queue)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "for i in range(100):\n",
    "    key, value = sess.run([key_op, value_op])\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Linear regression\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "\n",
    "x_train = np.linspace(-1, 1, 101)\n",
    "y_train = 2 * x_train + np.random.randn(*x_train.shape) * 0.33\n",
    "\n",
    "## y=wx\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "w = tf.Variable(0.0, name=\"weights\")\n",
    "\n",
    "def model(X, w):\n",
    "    return tf.multiply(X, w)\n",
    "\n",
    "y_model = model(X, w)\n",
    "cost = tf.reduce_mean(tf.square(Y-y_model))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for (x, y) in zip(x_train, y_train):\n",
    "        sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "## 经过学习 w_val = 1.91576    y_learned = x_train*w_val   \n",
    "w_val = sess.run(w) \n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Polynomial regression TensorFlow-Book-master/ch03_regression/Concept02_poly_regression.ipynb\n",
    "for i in range(num_coeffs):\n",
    "    trY += trY_coeffs[i] * np.power(trX, i)\n",
    "# 添加噪音\n",
    "trY += np.random.randn(*trX.shape) * 1.5\n",
    "\n",
    "def model(X, w):\n",
    "    terms = []\n",
    "    for i in range(num_coeffs):\n",
    "        term = tf.multiply(w[i], tf.pow(X, i))\n",
    "        terms.append(term)\n",
    "    return tf.add_n(terms)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for (x, y) in zip(trX, trY):\n",
    "        sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "\n",
    "w_val = sess.run(w)\n",
    "print(w_val)\n",
    "# 培训出来的是数组\n",
    "[ 1.10158885  2.36433625  3.30378437  4.43473864  3.75751448  4.60356045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b2b2b9b7f1dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 伪造数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_coeffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_dataset_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_coeffs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## 伪造数据\n",
    "x_dataset = np.linspace(-1, 1, 100)\n",
    "\n",
    "num_coeffs = 9\n",
    "y_dataset_params = [0.] * num_coeffs\n",
    "y_dataset_params[2] = 1\n",
    "y_dataset = 0\n",
    "for i in range(num_coeffs):\n",
    "    y_dataset += y_dataset_params[i] * np.power(x_dataset, i)\n",
    "y_dataset += np.random.randn(*x_dataset.shape) * 0.3\n",
    "\n",
    "print(y_dataset.shape)\n",
    "\n",
    "## 将数据打乱 重新分割\n",
    "def split_dataset(x_dataset, y_dataset, ratio):\n",
    "    arr = np.arange(x_dataset.size)\n",
    "    ## 打乱\n",
    "    np.random.shuffle(arr)\n",
    "    num_train = int(ratio * x_dataset.size)\n",
    "    ##重新分配\n",
    "    x_train = x_dataset[arr[0:num_train]]\n",
    "    y_train = y_dataset[arr[0:num_train]]\n",
    "    x_test = x_dataset[arr[num_train:x_dataset.size]]\n",
    "    y_test = y_dataset[arr[num_train:x_dataset.size]]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = split_dataset(x_dataset, y_dataset, 0.7)\n",
    "\n",
    "w = tf.Variable([0.] * num_coeffs, name=\"parameters\")\n",
    "y_model = model(X, w)\n",
    "\n",
    "## cost 函数，reg_lambda是程序变量 而不是tf变量\n",
    "cost = tf.div(tf.add(tf.reduce_sum(tf.square(Y-y_model)),\n",
    "                     tf.multiply(reg_lambda, tf.reduce_sum(tf.square(w)))),2*x_train.size)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "## reg_lambda 的使用，注意x_train 和 X: x_test\n",
    "for reg_lambda in np.linspace(0,1,100):\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(train_op, feed_dict={X: x_train, Y: y_train})\n",
    "    final_cost = sess.run(cost, feed_dict={X: x_test, Y:y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 评价  TensorFlow-Book-master/ch04_classification/Concept01_linear_regression_classification.ipynb\n",
    "y_model = model(X, w)\n",
    "cost = tf.reduce_sum(tf.square(Y-y_model))\n",
    "\n",
    "correct_prediction = tf.equal(Y, tf.to_float(tf.greater(y_model, 0.5)))\n",
    "accuracy = tf.reduce_mean(tf.to_float(correct_prediction))\n",
    "\n",
    "w_val = sess.run(w)\n",
    "print('learned parameters', w_val)\n",
    "print('accuracy', sess.run(accuracy, feed_dict={X: xs, Y: labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 逻辑回归 TensorFlow-Book-master/ch04_classification/Concept02_logistic.ipynb\n",
    "X = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\n",
    "w = tf.Variable([0., 0.], name=\"parameter\", trainable=True)\n",
    "## 定义公式\n",
    "y_model = tf.sigmoid(w[1] * X + w[0])\n",
    "cost = tf.reduce_mean(-Y * tf.log(y_model) - (1 - Y) * tf.log(1 - y_model))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "## 注意 打印日志 y_model,cost，op操作类型是没有值得只是操作\n",
    " with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    prev_err = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        y_model1,err, _ = sess.run([y_model,cost, train_op], {X: xs, Y: ys})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, err,y_model1[y_model1.size-1])\n",
    "        if abs(prev_err - err) < 0.0001:\n",
    "            break\n",
    "        prev_err = err\n",
    "    w_val = sess.run(w, {X: xs, Y: ys})\n",
    "\n",
    "\n",
    "all_xs = np.linspace(-10, 10, 100)\n",
    "with tf.Session() as sess:\n",
    "    ## w_val[1] + w_val[0] 是上一个值 可以直接来用的\n",
    "    predicted_vals = sess.run(tf.sigmoid(all_xs * w_val[1] + w_val[0]))\n",
    "plt.plot(all_xs, predicted_vals)\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## http://blog.csdn.net/qingyang666/article/details/70285249?locationNum=6&fps=1  softmax和k元分类区别\n",
    "## TensorFlow-Book-master/ch04_classification/Concept04_softmax.ipynb\n",
    "### hstack 水平加列，横着加。 vstack垂直加 就是行\n",
    "xs_label0 = np.hstack((x1_label0, x2_label0)) # x1_label0 (100, 1)\n",
    "xs_label1 = np.hstack((x1_label1, x2_label1))\n",
    "xs_label2 = np.hstack((x1_label2, x2_label2))\n",
    "\n",
    "print(xs_label0.shape)#(100, 2)\n",
    "\n",
    "xs = np.vstack((xs_label0, xs_label1, xs_label2))\n",
    "\n",
    "print(xs.shape)#(300, 2)\n",
    "\n",
    "labels = np.matrix([[1., 0., 0.]] * len(x1_label0) + [[0., 1., 0.]] * len(x1_label1) + [[0., 0., 1.]] * len(x1_label2))\n",
    "\n",
    "print(labels.shape )#(300, 3)\n",
    "\n",
    "test_xs = np.vstack((test_xs_label0, test_xs_label1, test_xs_label2))\n",
    "test_labels = np.matrix([[1., 0., 0.]] * 10 + [[0., 1., 0.]] * 10 + [[0., 0., 1.]] * 10)\n",
    "print([[1., 0., 0.]] * 2 +[[0., 1., 0.]] * 3)\n",
    "print(test_labels.shape)\n",
    "#[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]]\n",
    "#(30, 3)\n",
    "\n",
    "train_size, num_features = xs.shape\n",
    "print(xs.shape)#(300, 2)\n",
    "\n",
    "X = tf.placeholder(\"float\", shape=[None, num_features])\n",
    "Y = tf.placeholder(\"float\", shape=[None, num_labels])#num_labels = 3\n",
    "\n",
    "W = tf.Variable(tf.zeros([num_features, num_labels]))#shape[2,3]\n",
    "b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "y_model = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_sum(Y * tf.log(y_model))\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(training_epochs * train_size //batch_size):\n",
    "        offset = (step * batch_size) % train_size\n",
    "        batch_xs = xs[offset:(offset + batch_size), :]\n",
    "        batch_labels = labels[offset:(offset + batch_size)]\n",
    "        err, _ = sess.run([cost, train_op], feed_dict={X: batch_xs, Y: batch_labels})\n",
    "        if step % 100 == 0:\n",
    "            print (step, err)\n",
    "\n",
    "    W_val = sess.run(W)\n",
    "    print('w', W_val)\n",
    "    b_val = sess.run(b)\n",
    "    print('b', b_val)\n",
    "    ## 最后两句写法一样\n",
    "    print(\"accuracy\", accuracy.eval(feed_dict={X: test_xs, Y: test_labels}))\n",
    "    print(\"accuracy\",sess.run(accuracy,feed_dict={X: test_xs, Y: test_labels}) )\n",
    "    \"\"\"\n",
    "    w [[-2.47749233  0.20790951  2.26958179]\n",
    "       [-0.43322     1.92567241 -1.49245799]]\n",
    "    b [ 10.82922268  -2.62551188  -8.20370579]\n",
    "    accuracy 0.966667\n",
    "    accuracy 0.966667 \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.asarray([[[1,2,3],[4,5,6]],\n",
    "                [[7,8,9],[10,11,12]]])\n",
    "x_p = tf.placeholder(tf.int32,[2,2,3])\n",
    "y =  tf.reduce_sum(x_p,2) #修改这里\n",
    "y1 =  tf.reduce_sum(x_p,1) #修改这里\n",
    "y0 =  tf.reduce_sum(x_p,0) #修改这里\n",
    "with tf.Session() as sess:\n",
    "    y = sess.run(y,feed_dict={x_p:x})\n",
    "    y1 = sess.run(y1,feed_dict={x_p:x})\n",
    "    y0 = sess.run(y0,feed_dict={x_p:x})\n",
    "    print (\"y2: \",y)\n",
    "    print (\"y1: \",y1)\n",
    "    print (\"y0: \",y0)\n",
    "\"\"\"\n",
    "y2:  [[ 6 15]\n",
    "      [24 33]]\n",
    "y1:  [[ 5  7  9]\n",
    "      [17 19 21]]\n",
    "y0:  [[ 8 10 12]\n",
    "      [14 16 18]]\n",
    "\"\"\"    \n",
    "## 聚类计算 TensorFlow-Book-master/ch05_clustering/Concept01_clustering.ipynb\n",
    "def initial_cluster_centroids(X, k):\n",
    "    return X[0:k, :]\n",
    "\n",
    "\n",
    "def assign_cluster(X, centroids):\n",
    "    expanded_vectors = tf.expand_dims(X, 0)\n",
    "    expanded_centroids = tf.expand_dims(centroids, 1)\n",
    "    distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)\n",
    "    mins = tf.argmin(distances, 0)\n",
    "    return mins\n",
    "\n",
    "\n",
    "def recompute_centroids(X, Y):\n",
    "    sums = tf.unsorted_segment_sum(X, Y, k)\n",
    "    counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)\n",
    "    return sums / counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.1]]\n",
      "[[ 0.3 ]\n",
      " [ 0.04]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "slice_location = [0, 0] ##[0,0]\n",
    "slice_shape = [2, 1] ##[2,1]\n",
    "obs_prob = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n",
    "initial_prob = np.array([[0.6], [0.4]])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.slice(obs_prob, slice_location, slice_shape)))\n",
    "    fwd = tf.multiply(initial_prob, tf.slice(obs_prob, slice_location, slice_shape))\n",
    "    print(sess.run(fwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HMM TensorFlow-Book-master/ch06_hmm/Concept01_forward.ipynb 使用了递归算法 反推的使用\n",
    "# 还有一个类的使用 http://www.52nlp.cn/hmm-concrete-example-on-wiki\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class HMM(object):\n",
    "    def __init__(self, initial_prob, trans_prob, obs_prob):\n",
    "        self.N = np.size(initial_prob)\n",
    "        self.initial_prob = initial_prob\n",
    "        self.trans_prob = trans_prob\n",
    "        self.emission = tf.constant(obs_prob)\n",
    "\n",
    "        assert self.initial_prob.shape == (self.N, 1)\n",
    "        assert self.trans_prob.shape == (self.N, self.N)\n",
    "        assert obs_prob.shape[0] == self.N\n",
    "\n",
    "        self.obs_idx = tf.placeholder(tf.int32)\n",
    "        self.fwd = tf.placeholder(tf.float64)\n",
    "\n",
    "    def get_emission(self, obs_idx):\n",
    "        slice_location = [0, obs_idx] ##[0,0]\n",
    "        num_rows = tf.shape(self.emission)[0] ##2\n",
    "        slice_shape = [num_rows, 1] ##[2,1]\n",
    "        return tf.slice(self.emission, slice_location, slice_shape)\n",
    "\n",
    "    def forward_init_op(self):\n",
    "        obs_prob = self.get_emission(self.obs_idx)#[[ 0.5] [ 0.1]]\n",
    "        fwd = tf.multiply(self.initial_prob, obs_prob)\n",
    "        return fwd\n",
    "\n",
    "    def forward_op(self):\n",
    "        transitions = tf.matmul(self.fwd, tf.transpose(self.get_emission(self.obs_idx)))\n",
    "        weighted_transitions = transitions * self.trans_prob\n",
    "        fwd = tf.reduce_sum(weighted_transitions, 0)\n",
    "        return tf.reshape(fwd, tf.shape(self.fwd))\n",
    "    \n",
    "def forward_algorithm(sess, hmm, observations):\n",
    "    \n",
    "    fwd = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs_idx: observations[0]})\n",
    "    \n",
    "    print(fwd)\n",
    "    \n",
    "    for t in range(1, len(observations)):\n",
    "        fwd = sess.run(hmm.forward_op(), feed_dict={hmm.obs_idx: observations[t], hmm.fwd: fwd})\n",
    "        print(\"fwd:\",fwd)\n",
    "    prob = sess.run(tf.reduce_sum(fwd))# [[ 0.00038915] [ 0.0002039 ]] 两个数相加\n",
    "    return prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initial_prob = np.array([[0.6], [0.4]])\n",
    "    trans_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "    obs_prob = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n",
    "\n",
    "    hmm = HMM(initial_prob=initial_prob, trans_prob=trans_prob, obs_prob=obs_prob)\n",
    "    \n",
    "    # 因为obs_prob 有三个状态 所以索引是 01 2 注意参看连接文章\n",
    "    observations = [0, 1, 1, 2, 1, 1, 1]\n",
    "    with tf.Session() as sess:\n",
    "        prob = forward_algorithm(sess, hmm, observations)\n",
    "        print('Probability of observing {} is {}'.format(observations, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TensorFlow-Book-master/ch07_autoencoder 无监督的分类\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_batch(X, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a]\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_dim, hidden_dim, epoch=500, batch_size=10, learning_rate=0.001):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define input placeholder\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n",
    "        \n",
    "        # Define variables\n",
    "        with tf.name_scope('encode'):\n",
    "            weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n",
    "            encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n",
    "        with tf.name_scope('decode'):\n",
    "            weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n",
    "            decoded = tf.matmul(encoded, weights) + biases\n",
    "\n",
    "        self.x = x\n",
    "        self.encoded = encoded\n",
    "        self.decoded = decoded\n",
    "\n",
    "        # Define cost function and training op\n",
    "        self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded))))\n",
    "\n",
    "        self.all_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded)), 1))\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # Define a saver op\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epoch):\n",
    "                for j in range(500):\n",
    "                    batch_data = get_batch(data, self.batch_size)\n",
    "                    l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data})\n",
    "                if i % 50 == 0:\n",
    "                    print('epoch {0}: loss = {1}'.format(i, l))\n",
    "                    self.saver.save(sess, './model.ckpt')\n",
    "            self.saver.save(sess, './model.ckpt')\n",
    "        \n",
    "    def test(self, data):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n",
    "        print('input', data)\n",
    "        print('compressed', hidden)\n",
    "        print('reconstructed', reconstructed)\n",
    "        return reconstructed\n",
    "\n",
    "    def get_params(self):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            weights, biases = sess.run([self.weights1, self.biases1])\n",
    "        return weights, biases\n",
    "\n",
    "    def classify(self, data, labels):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data})\n",
    "            reconstructed = reconstructed[0]\n",
    "            # loss = sess.run(self.all_loss, feed_dict={self.x: data})\n",
    "            print('data', np.shape(data))\n",
    "            print('reconstructed', np.shape(reconstructed))\n",
    "            loss = np.sqrt(np.mean(np.square(data - reconstructed), axis=1))\n",
    "            print('loss', np.shape(loss))\n",
    "            horse_indices = np.where(labels == 7)[0]\n",
    "            not_horse_indices = np.where(labels != 7)[0]\n",
    "            horse_loss = np.mean(loss[horse_indices])\n",
    "            not_horse_loss = np.mean(loss[not_horse_indices])\n",
    "            print('horse', horse_loss)\n",
    "            print('not horse', not_horse_loss)\n",
    "            return hidden[7,:]\n",
    "\n",
    "    def decode(self, encoding):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            reconstructed = sess.run(self.decoded, feed_dict={self.encoded: encoding})\n",
    "        img = np.reshape(reconstructed, (32, 32))\n",
    "        return img\n",
    "    \n",
    "    \n",
    "from sklearn import datasets\n",
    "\n",
    "hidden_dim = 1\n",
    "data = datasets.load_iris().data\n",
    "input_dim = len(data[0])\n",
    "ae = Autoencoder(input_dim, hidden_dim)\n",
    "ae.train(data)\n",
    "ae.test([[8, 4, 6, 2]])\n",
    "\n",
    "\"\"\"input [[8, 4, 6, 2]]\n",
    "compressed [[ 0.72223264]]\n",
    "reconstructed [[ 6.87640762  2.79334426  6.23228502  2.21386957]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 自定义类 TensorFlow-Book-master/ch08_rl/Concept01_rl.ipynb\n",
    "## TensorFlow-Book-master/ch09_cnn/Concept02_convolution CNN 查看单个图\n",
    "# TensorFlow-Book-master/ch09_cnn/Concept03_cnn.ipynb\n",
    "model_op = model()\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=model_op, labels=y)\n",
    ")\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow-Book-master/ch10_rnn/Concept02_rnn.ipynb RNN\n",
    "#  tf.get_variable_scope().reuse_variables()的使用\n",
    "# BasicLSTMCell 的使用  打开文件下载文件\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "class SeriesPredictor:\n",
    "\n",
    "    def __init__(self, input_dim, seq_size, hidden_dim=10):# 1,4,10\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_size = seq_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Weight variables and input placeholders\n",
    "        self.W_out = tf.Variable(tf.random_normal([hidden_dim, 1]), name='W_out')#[10,1]\n",
    "        self.b_out = tf.Variable(tf.random_normal([1]), name='b_out')\n",
    "        self.x = tf.placeholder(tf.float32, [None, seq_size, input_dim])#[#,4,1]\n",
    "        self.y = tf.placeholder(tf.float32, [None, seq_size])#[#,4]\n",
    "\n",
    "        # Cost optimizer\n",
    "        self.cost = tf.reduce_mean(tf.square(self.model() - self.y))\n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(self.cost)\n",
    "\n",
    "        # Auxiliary ops\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        :param x: inputs of size [T, batch_size, input_size]\n",
    "        :param W: matrix of fully-connected output layer weights\n",
    "        :param b: vector of fully-connected output layer biases\n",
    "        \"\"\"\n",
    "        cell = rnn.BasicLSTMCell(self.hidden_dim)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32)\n",
    "        num_examples = tf.shape(self.x)[0]\n",
    "        W_repeated = tf.tile(tf.expand_dims(self.W_out, 0), [num_examples, 1, 1])\n",
    "        out = tf.matmul(outputs, W_repeated) + self.b_out\n",
    "        out = tf.squeeze(out)\n",
    "        return out\n",
    "\n",
    "    def train(self, train_x, train_y):\n",
    "        with tf.Session() as sess:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(1000):\n",
    "                _, mse = sess.run([self.train_op, self.cost], feed_dict={self.x: train_x, self.y: train_y})\n",
    "                if i % 100 == 0:\n",
    "                    print(i, mse)\n",
    "            save_path = self.saver.save(sess, 'model.ckpt')\n",
    "            print('Model saved to {}'.format(save_path))\n",
    "\n",
    "    def test(self, test_x):\n",
    "        with tf.Session() as sess:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.saver.restore(sess, './model.ckpt')\n",
    "            output = sess.run(self.model(), feed_dict={self.x: test_x})\n",
    "            return output\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    predictor = SeriesPredictor(input_dim=1, seq_size=4, hidden_dim=10)\n",
    "    train_x = [[[1], [2], [5], [6]],\n",
    "               [[5], [7], [7], [8]],\n",
    "               [[3], [4], [5], [7]]]\n",
    "    train_y = [[1, 3, 7, 11],\n",
    "               [5, 12, 14, 15],\n",
    "               [3, 7, 9, 12]]\n",
    "    predictor.train(train_x, train_y)\n",
    "\n",
    "    test_x = [[[1], [2], [3], [4]],  # 1, 3, 5, 7\n",
    "              [[4], [5], [6], [7]]]  # 4, 9, 11, 13\n",
    "    actual_y = [[[1], [3], [5], [7]],\n",
    "                [[4], [9], [11], [13]]]\n",
    "    pred_y = predictor.test(test_x)\n",
    "    \n",
    "    print(\"\\nLets run some tests!\\n\")\n",
    "    \n",
    "    for i, x in enumerate(test_x):\n",
    "        print(\"When the input is {}\".format(x))\n",
    "        print(\"The ground truth output should be {}\".format(actual_y[i]))\n",
    "        print(\"And the model thinks it is {}\\n\".format(pred_y[i])) ## 测试完的值 就是预测的输出\n",
    "        \n",
    "\"\"\"Lets run some tests!\n",
    "\n",
    "When the input is [[1], [2], [3], [4]]\n",
    "The ground truth output should be [[1], [3], [5], [7]]\n",
    "And the model thinks it is [ 0.96018004  2.76944828  5.35826826  7.3706851 ]\n",
    "\n",
    "When the input is [[4], [5], [6], [7]]\n",
    "The ground truth output should be [[4], [9], [11], [13]]\n",
    "And the model thinks it is [  4.17302942   9.161376    11.13204765  11.64120388]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TensorFlow-Book-master/ch11_word2vec/Concept01_multi_rnn.ipynb\n",
    "import tensorflow as tf\n",
    "input_dim = 1\n",
    "seq_size = 3\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, seq_size, input_dim])\n",
    "\n",
    "def make_cell(state_dim):\n",
    "    return tf.contrib.rnn.LSTMCell(state_dim)\n",
    "\n",
    "with tf.variable_scope(\"first_cell\") as scope:\n",
    "    cell = make_cell(state_dim=10)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, input_placeholder, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"second_cell\") as scope:\n",
    "    cell2 = make_cell(state_dim=10)\n",
    "    outputs2, states2 = tf.nn.dynamic_rnn(cell2, outputs, dtype=tf.float32)\n",
    "\n",
    "def make_multi_cell(state_dim, num_layers):\n",
    "    cells = [make_cell(state_dim) for _ in range(num_layers)]\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "multi_cell = make_multi_cell(state_dim=10, num_layers=5)\n",
    "outputs5, states5 = tf.nn.dynamic_rnn(multi_cell, input_placeholder, dtype=tf.float32)\n",
    "\n",
    "input_seq = [[1], [2], [3]]\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "\n",
    "outputs_val, outputs2_val, outputs5_val = sess.run([outputs, outputs2, outputs5], \n",
    "                                                   feed_dict={input_placeholder: [input_seq]})\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow-Book-master/ch11_word2vec/Concept02_embedding_lookup.ipynb\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "embeddings_0d = tf.constant([17,22,35,51])\n",
    "\n",
    "embeddings_4d = tf.constant([[1, 0, 0, 0],\n",
    "                             [0, 1, 0, 0],\n",
    "                             [0, 0, 1, 0],\n",
    "                             [0, 0, 0, 1]])\n",
    "\n",
    "embeddings_2x2d = tf.constant([[[1, 0], [0, 0]],\n",
    "                               [[0, 1], [0, 0]],\n",
    "                               [[0, 0], [1, 0]],\n",
    "                               [[0, 0], [0, 1]]])\n",
    "\n",
    "ids = tf.constant([1, 0, 2])\n",
    "\n",
    "lookup_0d = sess.run(tf.nn.embedding_lookup(embeddings_0d, ids))\n",
    "print(lookup_0d) #[22 17 35]\n",
    "\n",
    "lookup_4d = sess.run(tf.nn.embedding_lookup(embeddings_4d, ids))\n",
    "print(lookup_4d)\n",
    "\n",
    "\"\"\"\n",
    "[[0 1 0 0]\n",
    " [1 0 0 0]\n",
    " [0 0 1 0]]\n",
    "\"\"\"\n",
    "lookup_2x2d = sess.run(tf.nn.embedding_lookup(embeddings_2x2d, ids))\n",
    "print(lookup_2x2d)\n",
    "\n",
    "\"\"\"\n",
    "[[[0 1]\n",
    "  [0 0]]\n",
    "\n",
    " [[1 0]\n",
    "  [0 0]]\n",
    "\n",
    " [[0 0]\n",
    "  [1 0]]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  https://hackernoon.com/deep-learning-cnns-in-tensorflow-with-gpus-cba6efe0acc2 完整的例子，评估 恢复数据  类\n",
    "\n",
    "# 归一化\n",
    "def normalize(x):\n",
    "   \n",
    "    normalized_x = ( x - x.min())  /( x.max() - x.min() )\n",
    "    return normalized_x\n",
    "\n",
    "#tf.train.import_meta_graph  参看下面的连接另一种保存 \n",
    "#另一种保存 还原方式 https://github.com/vyomshm/Image-Classification-Project/blob/master/dlnd_image_classification.ipynb 比较商业的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.bonaccorso.eu/2017/08/02/svd-recommendations-using-tensorflow/  推荐系统归类\n",
    "# https://blog.altoros.com/using-k-means-clustering-in-tensorflow.html 非监督学习 无须标签 K聚类\n",
    "# https://www.altoros.com/performance-benchmark-distributed-tensorflow.html 分布式多核配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.slice http://blog.csdn.net/chenxieyy/article/details/53031943  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x=[[1,2,3],[4,5,6]]\n",
    "y=np.arange(24).reshape([2,3,4])\n",
    "z=tf.constant([[[1,2,3],[4,5,6]], [[7,8,9],[10,11,12]],  [[13,14,15],[16,17,18]]]\n",
    "sess=tf.Session()\n",
    "begin_x=[1,0]        #第一个1，决定了从x的第二行[4,5,6]开始，第二个0，决定了从[4,5,6] 中的4开始抽取\n",
    "size_x=[1,2]           # 第一个1决定了，从第二行以起始位置抽取1行，也就是只抽取[4,5,6] 这一行，在这一行中从4开始抽取2个元素\n",
    "out=tf.slice(x,begin_x,size_x)\n",
    "print sess.run(out)  #  结果:[[4 5]]\n",
    "\n",
    "begin_y=[1,0,0]\n",
    "size_y=[1,2,3]\n",
    "out=tf.slice(y,begin_y,size_y)   \n",
    "print sess.run(out)  # 结果:[[[12 13 14] [16 17 18]]]\n",
    "\n",
    "begin_z=[0,1,1]\n",
    "size_z=[-1,1,2] \n",
    "out=tf.slice(z,begin_z,size_z)\n",
    "print sess.run(out)  # size[i]=-1 表示第i维从begin[i]剩余的元素都要被抽取，结果：[[[ 5  6]] [[11 12]] [[17 18]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.gather   https://blog.altoros.com/using-k-means-clustering-in-tensorflow.html\n",
    "import tensorflow as tf\n",
    "\n",
    "temp = tf.range(0,10)*10 + tf.constant(1,shape=[10])\n",
    "temp2 = tf.gather(temp,[1,5,9])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    print sess.run(temp)\n",
    "    print sess.run(temp2)\n",
    "\n",
    "\"\"\"\n",
    "[ 1 11 21 31 41 51 61 71 81 91] \n",
    "[11 51 91]\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an overview of the 5 steps in the LSTM model life-cycle in Keras that we are going to look at.\n",
    "\n",
    "Define Network\n",
    "Compile Network\n",
    "Fit Network\n",
    "Evaluate Network\n",
    "Make Predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 下面链接是conda 环境配置 更新，版本查看\n",
    "## http://webcache.googleusercontent.com/search?q=cache:http://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/\n",
    "\n",
    "## 销量预测 讲解了 划分数据  数据归一化\n",
    "## LSTM http://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "## LSTM 数据准备  天气预测\n",
    "## http://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "\n",
    "\"\"\"http://localhost:8888/notebooks/AnacondaProjects/TensorFlowStudy.ipynb#\n",
    "http://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n",
    "Keras Classification Metrics\n",
    "\n",
    "Below is a list of the metrics that you can use in Keras on classification problems.\n",
    "\n",
    "Binary Accuracy: binary_accuracy, acc\n",
    "Categorical Accuracy: categorical_accuracy, acc\n",
    "Sparse Categorical Accuracy: sparse_categorical_accuracy\n",
    "Top k Categorical Accuracy: top_k_categorical_accuracy (requires you specify a k parameter)\n",
    "Sparse Top k Categorical Accuracy: sparse_top_k_categorical_accuracy (requires you specify a k parameter)\n",
    "Accuracy is special.\n",
    "\n",
    "Regardless of whether your problem is a binary or multi-class classification problem, you can specify the ‘acc‘ metric to report on accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归 loss 函数  使用单独的公式\n",
    "\n",
    "# L1-Loss\n",
    "--------------\n",
    "\n",
    "Here, we will illustrate linear regression with the L1-Loss. Later in this script, we will illustrate the same problem with L2-Loss.\n",
    "\n",
    "The equation for the L1 Loss for Linear Least Squares is\n",
    "\n",
    "$$S = \\sum_{i=1}^{N}\\left| y_{i} - \\hat{y_{i}} \\right|$$\n",
    "\n",
    "Where $N$ is the number of data points, $y_{i}$ is the i-th actual y-values, $\\hat{y_{i}}$ is the predicted i-th y-value.\n",
    "\n",
    "We start a computational graph session.\n",
    "\n",
    "# L2-Loss\n",
    "--------\n",
    "\n",
    "Here, we will illustrate linear regression with the L2-Loss..\n",
    "\n",
    "The equation for the L2 Loss for Linear Least Squares is\n",
    "\n",
    "$$S = \\sum_{i=1}^{N}\\left( y_{i} - \\hat{y_{i}} \\right)^{2}$$\n",
    "\n",
    "Where $N$ is the number of data points, $y_{i}$ is the i-th actual y-values, $\\hat{y_{i}}$ is the predicted i-th y-value.\n",
    "\n",
    "We start a computational graph session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow_cookbook-master/03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions/04_lin_reg_l1_vs_l2.ipynb\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "# Declare loss functions\n",
    "loss_l1 = tf.reduce_mean(tf.abs(y_target - model_output))\n",
    "\n",
    "# Declare optimizers\n",
    "my_opt_l1 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step_l1 = my_opt_l1.minimize(loss_l1)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#______________________________________________\n",
    "\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "# Declare loss functions\n",
    "loss_l2 = tf.reduce_mean(tf.square(y_target - model_output))\n",
    "\n",
    "# Declare optimizers\n",
    "my_opt_l2 = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step_l2 = my_opt_l2.minimize(loss_l2)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deming Regression\n",
    "-------------------------------\n",
    "\n",
    "This function shows how to use TensorFlow to solve linear Deming regression.\n",
    "\n",
    "$y = Ax + b$\n",
    "\n",
    "We will use the iris data, specifically:\n",
    "\n",
    "y = Sepal Length and x = Petal Width.\n",
    "\n",
    "Demming regression is also called total least squares, in which we minimize the shortest distance from the predicted line and the actual (x,y) points.\n",
    "\n",
    "If least squares linear regression minimizes the vertical distance to the line, Deming regression minimizes the total distance to the line.  This type of regression minimizes the error in the y values and the x values.  See the below figure for a comparison.\n",
    "\n",
    "<img src=\"../images/05_demming_vs_linear_reg.png\" width=\"512\">\n",
    "\n",
    "To implement this in TensorFlow, we start by loading the necessary libraries.\n",
    "\n",
    "For the demming loss, we want to compute:\n",
    "\n",
    "$$ \\frac{\\left| A \\cdot x + b - y \\right|}{\\sqrt{A^{2} + 1}} $$\n",
    "\n",
    "Which will give us the shortest distance between a point (x,y) and the predicted line, $A \\cdot x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/03_Linear_Regression/05_Implementing_Deming_Regression/05_demming_regression.ipynb\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "# Declare Demming loss function\n",
    "demming_numerator = tf.abs(tf.subtract(tf.add(tf.matmul(x_data, A), b), y_target))\n",
    "demming_denominator = tf.sqrt(tf.add(tf.square(A),1))\n",
    "loss = tf.reduce_mean(tf.truediv(demming_numerator, demming_denominator))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.25)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-df4cbe2bb28e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# tensorflow_cookbook-master/03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression/06_lasso_and_ridge_regression.ipynb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Declare model operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Select appropriate loss function based on regression type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# tensorflow_cookbook-master/03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression/06_lasso_and_ridge_regression.ipynb\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "# Select appropriate loss function based on regression type\n",
    "\n",
    "if regression_type == 'LASSO':\n",
    "    # Declare Lasso loss function\n",
    "    # Lasso Loss = L2_Loss + heavyside_step,\n",
    "    # Where heavyside_step ~ 0 if A < constant, otherwise ~ 99\n",
    "    lasso_param = tf.constant(0.9)\n",
    "    heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param)))))\n",
    "    regularization_param = tf.multiply(heavyside_step, 99.)\n",
    "    loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param)\n",
    "\n",
    "elif regression_type == 'Ridge':\n",
    "    # Declare the Ridge loss function\n",
    "    # Ridge loss = L2_loss + L2 norm of slope\n",
    "    ridge_param = tf.constant(1.)\n",
    "    ridge_loss = tf.reduce_mean(tf.square(A))\n",
    "    loss = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), 0)\n",
    "    \n",
    "else:\n",
    "    print('Invalid regression_type parameter value',file=sys.stderr)\n",
    "    \n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow_cookbook-master/03_Linear_Regression/07_Implementing_Elasticnet_Regression/07_elasticnet_regression.ipynb\n",
    "# Create variables for linear regression\n",
    "A = tf.Variable(tf.random_normal(shape=[3,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare the elastic net loss function\n",
    "elastic_param1 = tf.constant(1.)\n",
    "elastic_param2 = tf.constant(1.)\n",
    "l1_a_loss = tf.reduce_mean(tf.abs(A))\n",
    "l2_a_loss = tf.reduce_mean(tf.square(A))\n",
    "e1_term = tf.multiply(elastic_param1, l1_a_loss)\n",
    "e2_term = tf.multiply(elastic_param2, l2_a_loss)\n",
    "loss = tf.expand_dims(tf.add(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), e1_term), e2_term), 0)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow_cookbook-master/03_Linear_Regression/08_Implementing_Logistic_Regression/08_logistic_regression.ipynb\n",
    "# 逻辑回归  数据处理的知识，下载解析\n",
    "\n",
    "# Declare model operations\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Actual Prediction 预测的方法 统计\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This function shows how to use TensorFlow to solve logistic regression.\n",
    "$ \\textbf{y} = sigmoid(\\textbf{A}\\times \\textbf{x} + \\textbf{b})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#/tensorflow_cookbook-master/04_Support_Vector_Machines/02_Working_with_Linear_SVMs/02_linear_svm.ipynb\n",
    "# 所有的tensorflow_cookbook-master 的readme 都有图做说明，这个打不开 请在github中打开查看 很有用\n",
    "# Declare model operations\n",
    "model_output = tf.subtract(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare vector L2 'norm' function squared\n",
    "l2_norm = tf.reduce_sum(tf.square(A))\n",
    "\n",
    "# Declare loss function\n",
    "# Loss = max(0, 1-pred*actual) + alpha * L2_norm(A)^2\n",
    "# L2 regularization parameter, alpha\n",
    "alpha = tf.constant([0.01])\n",
    "# Margin term in loss\n",
    "classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., tf.multiply(model_output, y_target))))\n",
    "# Put terms together\n",
    "loss = tf.add(classification_term, tf.multiply(alpha, l2_norm))\n",
    "\n",
    "# Declare prediction function\n",
    "prediction = tf.sign(model_output)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare our model and L2 Norm\n",
    "\n",
    "SVM linear model is given by the equation:\n",
    "\n",
    "$$\\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\max(0, 1 - A \\cdot x - b) \\right] + \\alpha \\cdot ||A||^{2}$$\n",
    "\n",
    "Our loss function will be the above quantity and we will tell TensorFlow to minimize it. Note that $n$ is the number of points (in a batch), $A$ is the hyperplane-normal vector (to solve for), $b$ is the hyperplane-offset (to solve for), and $\\alpha$ is the soft-margin parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow_cookbook-master/04_Support_Vector_Machines/04_Working_with_Kernels/04_svm_kernels.ipynb 不是线性的 可以参看图，公式比较复杂\n",
    "# Apply kernel\n",
    "# Linear Kernel\n",
    "# my_kernel = tf.matmul(x_data, tf.transpose(x_data))\n",
    "\n",
    "# Gaussian (RBF) kernel\n",
    "gamma = tf.constant(-50.0)\n",
    "dist = tf.reduce_sum(tf.square(x_data), 1)\n",
    "dist = tf.reshape(dist, [-1,1])\n",
    "sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))), tf.transpose(dist))\n",
    "my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))\n",
    "\n",
    "# Compute SVM Model\n",
    "first_term = tf.reduce_sum(b)\n",
    "b_vec_cross = tf.matmul(tf.transpose(b), b)\n",
    "y_target_cross = tf.matmul(y_target, tf.transpose(y_target))\n",
    "second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\n",
    "loss = tf.negative(tf.subtract(first_term, second_term))\n",
    "\n",
    "# Create Prediction Kernel\n",
    "# Linear prediction kernel\n",
    "# my_kernel = tf.matmul(x_data, tf.transpose(prediction_grid))\n",
    "\n",
    "# Gaussian (RBF) prediction kernel\n",
    "rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\n",
    "rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
    "pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n",
    "\n",
    "prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b), pred_kernel)\n",
    "prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 非线性 多元分类都支持\n",
    "tensorflow_cookbook-master/04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs/06_multiclass_svm.ipynb\n",
    "tensorflow_cookbook-master/04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs/05_nonlinear_svm.ipynb\n",
    "\n",
    "### Gaussian (RBF) Kernel\n",
    "\n",
    "We create the gaussian kernel that is used to transform the data points into a higher dimensional space.\n",
    "\n",
    "The Kernel of two points, $x$ and $x'$ is given as\n",
    "\n",
    "$$K(x, x')=exp\\left(-\\gamma|| x-x' ||^{2}\\right)$$\n",
    "\n",
    "For $\\gamma$ very small, the kernel is very wide, and vice-versa for large $\\gamma$ values.  This means that large $\\gamma$ leads to high bias and low variance models.\n",
    "\n",
    "If we have a vector of points, $x$ of size (batch_size, 2), then our kernel calculation becomes\n",
    "\n",
    "$$K(\\textbf{x})=exp\\left( -\\gamma \\textbf{x} \\cdot \\textbf{x}^{T} \\right)$$\n",
    "\n",
    "### Compute SVM Model\n",
    "\n",
    "Here, the SVM loss is given by two terms, The first term is the sum of the $b$ matrix, and the second term is \n",
    "\n",
    "$$\\sum\\left(K\\cdot||\\textbf{b}||^{2}||\\textbf{y}||^{2}\\right)$$\n",
    "\n",
    "We finally tell TensorFlow to maximize the loss by minimizing the negative:  (The following is a horribly abbreviated version of the dual problem)\n",
    "\n",
    "$$-\\left(\\sum\\textbf{b} - \\sum\\left(K\\cdot||\\textbf{b}||^{2}||\\textbf{y}||^{2}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian (RBF) kernel\n",
    "gamma = tf.constant(-50.0)\n",
    "sq_vec = tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))\n",
    "my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_vec)))\n",
    "\n",
    "# Compute SVM Model\n",
    "first_term = tf.reduce_sum(b)\n",
    "b_vec_cross = tf.matmul(tf.transpose(b), b)\n",
    "y_target_cross = tf.matmul(y_target, tf.transpose(y_target))\n",
    "second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\n",
    "loss = tf.negative(tf.subtract(first_term, second_term))\n",
    "\n",
    "# Gaussian (RBF) prediction kernel\n",
    "rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\n",
    "rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
    "pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n",
    "\n",
    "prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b), pred_kernel)\n",
    "prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
